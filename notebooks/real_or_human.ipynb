{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "436d0296-0adc-411d-a698-4c5c8f936ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.io import imread\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.feature import canny\n",
    "import numpy as np\n",
    "from gtda.homology import VietorisRipsPersistence\n",
    "from gtda.diagrams import PersistenceEntropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a8314c04-fbdd-44ad-9cb0-ed57e2ab92cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human dataset1:  Index(['image_id', '5_o_Clock_Shadow', 'Arched_Eyebrows', 'Attractive',\n",
      "       'Bags_Under_Eyes', 'Bald', 'Bangs', 'Big_Lips', 'Big_Nose',\n",
      "       'Black_Hair', 'Blond_Hair', 'Blurry', 'Brown_Hair', 'Bushy_Eyebrows',\n",
      "       'Chubby', 'Double_Chin', 'Eyeglasses', 'Goatee', 'Gray_Hair',\n",
      "       'Heavy_Makeup', 'High_Cheekbones', 'Male', 'Mouth_Slightly_Open',\n",
      "       'Mustache', 'Narrow_Eyes', 'No_Beard', 'Oval_Face', 'Pale_Skin',\n",
      "       'Pointy_Nose', 'Receding_Hairline', 'Rosy_Cheeks', 'Sideburns',\n",
      "       'Smiling', 'Straight_Hair', 'Wavy_Hair', 'Wearing_Earrings',\n",
      "       'Wearing_Hat', 'Wearing_Lipstick', 'Wearing_Necklace',\n",
      "       'Wearing_Necktie', 'Young'],\n",
      "      dtype='object')\n",
      "\n",
      "Human dataset2:  Index(['image_id', 'x_1', 'y_1', 'width', 'height'], dtype='object')\n",
      "\n",
      "Human dataset3:  Index(['image_id', 'partition'], dtype='object')\n",
      "\n",
      "Human dataset4:  Index(['image_id', 'lefteye_x', 'lefteye_y', 'righteye_x', 'righteye_y',\n",
      "       'nose_x', 'nose_y', 'leftmouth_x', 'leftmouth_y', 'rightmouth_x',\n",
      "       'rightmouth_y'],\n",
      "      dtype='object')\n",
      "\n",
      "AI dataset:  Index([    0,     1,     2,     3,     4,     5,     6,     7,     8,     9,\n",
      "       ...\n",
      "       69990, 69991, 69992, 69993, 69994, 69995, 69996, 69997, 69998, 69999],\n",
      "      dtype='int64', length=70000)\n",
      "\n",
      "Merged dataset:       image_id  5_o_Clock_Shadow  Arched_Eyebrows  Attractive  Bags_Under_Eyes  \\\n",
      "0  000001.jpg                -1                1           1               -1   \n",
      "1  000002.jpg                -1               -1          -1                1   \n",
      "2  000003.jpg                -1               -1          -1               -1   \n",
      "3  000004.jpg                -1               -1           1               -1   \n",
      "4  000005.jpg                -1                1           1               -1   \n",
      "\n",
      "   Bald  Bangs  Big_Lips  Big_Nose  Black_Hair  ...  lefteye_x  lefteye_y  \\\n",
      "0    -1     -1        -1        -1          -1  ...         69        109   \n",
      "1    -1     -1        -1         1          -1  ...         69        110   \n",
      "2    -1     -1         1        -1          -1  ...         76        112   \n",
      "3    -1     -1        -1        -1          -1  ...         72        113   \n",
      "4    -1     -1         1        -1          -1  ...         66        114   \n",
      "\n",
      "   righteye_x  righteye_y  nose_x  nose_y  leftmouth_x  leftmouth_y  \\\n",
      "0         106         113      77     142           73          152   \n",
      "1         107         112      81     135           70          151   \n",
      "2         104         106     108     128           74          156   \n",
      "3         108         108     101     138           71          155   \n",
      "4         112         112      86     119           71          147   \n",
      "\n",
      "   rightmouth_x  rightmouth_y  \n",
      "0           108           154  \n",
      "1           108           153  \n",
      "2            98           158  \n",
      "3           101           151  \n",
      "4           104           150  \n",
      "\n",
      "[5 rows x 56 columns]\n"
     ]
    }
   ],
   "source": [
    "human_data1 = pd.read_csv(\"../datasets/human/list_attr_celeba.csv\")\n",
    "human_data2 = pd.read_csv(\"../datasets/human/list_bbox_celeba.csv\")\n",
    "human_data3 = pd.read_csv(\"../datasets/human/list_eval_partition.csv\")\n",
    "human_data4 = pd.read_csv(\"../datasets/human/list_landmarks_align_celeba.csv\")\n",
    "\n",
    "# Display the columns in human datasets\n",
    "print(\"Human dataset1: \", human_data1.columns)\n",
    "print(\"\\nHuman dataset2: \", human_data2.columns)\n",
    "print(\"\\nHuman dataset3: \", human_data3.columns)\n",
    "print(\"\\nHuman dataset4: \", human_data4.columns)\n",
    "\n",
    "# Load AI dataset\n",
    "ai_datasets = pd.read_json(\"../datasets/ai/ffhq-dataset-v1-processed.json\")\n",
    "print(\"\\nAI dataset: \", ai_datasets.columns)\n",
    "\n",
    "# Merging human datasets\n",
    "merged_df = pd.merge(human_data1, human_data2, on='image_id').merge(human_data3, on='image_id').merge(human_data4, on='image_id')\n",
    "print(\"\\nMerged dataset: \", merged_df.head())\n",
    "\n",
    "# Label human data as 0 (real human)\n",
    "merged_df[\"label\"] = 0\n",
    "\n",
    "# Prepare AI data\n",
    "ai_df = pd.DataFrame({\n",
    "    \"image_id\" : [f\"{i}.png\" for i in range(len(ai_datasets))],\n",
    "    \"label\" : 1 # 1 for AI generated\n",
    "})\n",
    "\n",
    "# Combine human and AI dataframes\n",
    "human_df = merged_df[[\"image_id\", \"label\"]]\n",
    "combined_df = pd.concat([human_df, ai_df], ignore_index=True)\n",
    "combined_df = combined_df.sample(frac=1).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b5e3c0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "addbbb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, dataframe, transform=None, root_dir=\"../datasets\"):\n",
    "        self.dataframe = dataframe\n",
    "        self.transform = transform\n",
    "        self.root_dir = root_dir\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataframe.iloc[idx]\n",
    "        img_name = row[\"image_id\"]\n",
    "        label = row[\"label\"]\n",
    "\n",
    "        if label == 0:\n",
    "            img_path = os.path.join(self.root_dir, \"human\", \"img_align_celeba\", img_name)\n",
    "        else:\n",
    "            img_path = os.path.join(self.root_dir, \"ai\", \"faces_dataset\", img_name)\n",
    "        \n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label, img_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9b14967d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df = train_test_split(combined_df, test_size=0.2, stratify=combined_df['label'], random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "353c6f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ImageDataset(train_df, transform=transform)\n",
    "val_dataset = ImageDataset(val_df, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bd29187b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, 3, padding=1)  # 3 channels for RGB, 16 filters for 1st conv layer\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)  # 32 filters for 2nd conv layer\n",
    "        self.pool = nn.MaxPool2d(2, 2)  # 2x2 pooling\n",
    "        self.fc1 = nn.Linear(32 * 16 * 16, 128)  # 32 channels, 16x16 feature map after pooling\n",
    "        self.fc2 = nn.Linear(128 + 10, 1)  # 10 for TDA features, 1 output for binary classification\n",
    "\n",
    "    def forward(self, x, tda_features):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 32 * 16 * 16)  # Flatten the feature map\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = torch.cat((x, tda_features), dim=1)  # Combine CNN and TDA features\n",
    "        x = torch.sigmoid(self.fc2(x))  # Sigmoid activation for binary classification\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "72a6dbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SimpleCNN().to(device)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6c62dc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for images, labels in loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.float().unsqueeze(1).to(device)\n",
    "        tda_vecs = extract_tda_features(paths)  # Placeholder for TDA feature extraction\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images, tda_vecs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "def validate(model, loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.float().unsqueeze(1).to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            predicted = (outputs > 0.5).float()\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    accuracy = correct / total\n",
    "    return total_loss / len(loader), accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea11678",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train(model, train_loader, optimizer, criterion)\n",
    "    val_loss, val_acc = validate(model, val_loader, criterion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3e6738",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train(model, train_loader, optimizer, criterion)\n",
    "    val_loss, val_accuracy = validate(model, val_loader, criterion)\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accuracies.append(val_accuracy)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy*100:.2f}%\")\n",
    "\n",
    "# Plot Loss Curves\n",
    "plt.figure(figsize=(12,5))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(train_losses, label=\"Train Loss\")\n",
    "plt.plot(val_losses, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss over Epochs\")\n",
    "plt.legend()\n",
    "\n",
    "# Plot Accuracy Curve\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(val_accuracies, label=\"Validation Accuracy\", color='green')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Validation Accuracy over Epochs\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445b08de",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"simple_cnn_real_vs_ai.pth\")\n",
    "print(\"Model saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c129a35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image(image_path, model, transform):\n",
    "    model.eval()\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image = transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(image)\n",
    "        prediction = (output > 0.5).float()\n",
    "\n",
    "    if prediction.item() == 0:\n",
    "        print(f\"Prediction: Real Human Image ðŸ§‘â€ðŸ¦°\")\n",
    "    else:\n",
    "        print(f\"Prediction: AI Generated Image ðŸ¤–\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa84ca36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_to_point_cloud(image_path):\n",
    "    image = imread(image_path)\n",
    "    gray = rgb2gray(image)\n",
    "    edges = canny(gray) \n",
    "    points = np.column_stack(np.nonzero(edges))\n",
    "    return points\n",
    "\n",
    "VR = VietorisRipsPersistence(homology_dimensions=[0, 1])\n",
    "PE = PersistenceEntropy()\n",
    "\n",
    "def get_tda_vector(image_path):\n",
    "    gray = rgb2gray(imread(image_path)) \n",
    "    edges = canny(gray) \n",
    "    points = np.column_stack(np.nonzero(edges)).reshape(1, -1, 2)\n",
    "    diag = VR.fit_transform(points)\n",
    "    vec = PE.fit_transform(diag)\n",
    "    return vec.squeeze()\n",
    "\n",
    "def extract_tda_features(batch_paths):\n",
    "    vectors = [get_tda_vector(path) for path in batch_paths]\n",
    "    return torch.tensor(vectors, dtype=torch.float32).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d350e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, loader, criterion):\n",
    "    model.eval() # Set model to evaluation mode\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images = images.to(device) # Move images to GPU if available\n",
    "            labels = labels.float().unsqueeze(1).to(device) # Unsqueeze to add a dimension for binary classification\n",
    "            outputs = model(images) # Forward pass through the model\n",
    "            loss = criterion(outputs, labels) # Calculate loss\n",
    "            total_loss += loss.item() # Accumulate loss\n",
    "\n",
    "            predicted = (outputs > 0.5).float() # Convert probabilities to binary predictions\n",
    "            correct += (predicted == labels).sum().item() # Count correct predictions\n",
    "            total += labels.size(0) \n",
    "    accuracy = correct / total\n",
    "    return total_loss / len(loader), accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8cca44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
